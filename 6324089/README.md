# SoluÃ§Ã£o - Pipeline ETL de Produtos e Vendas

**Aluno:** Giovanna Sabino  
**RA:** 6324089  
**Data:** Novembro de 2025  
**Disciplina:** Engenharia de Dados - Pipeline de Dados

---

## ğŸ“‹ Sobre o Projeto

Este projeto implementa um pipeline ETL completo para processar dados de produtos e vendas de uma empresa de e-commerce, utilizando Apache Airflow para orquestraÃ§Ã£o e PostgreSQL para armazenamento dos dados.

---

## ğŸ¯ Parte 1: AnÃ¡lise e Planejamento

### Problemas Identificados nos Dados

#### Arquivo `produtos_loja.csv`:
- **Preco_Custo nulo:** Produto P003 (Teclado MecÃ¢nico)
- **Fornecedor nulo:** Produto P005 (Webcam HD)

#### Arquivo `vendas_produtos.csv`:
- **Preco_Venda nulo:** Venda V005 (10 unidades de Mouse Logitech)

### EstratÃ©gia: ETL (Extract, Transform, Load)

#### Justificativa da Escolha:

Optei pela abordagem **ETL** pelas seguintes razÃµes:

1. **Volume de Dados Reduzido:**
   - Apenas 5 produtos e 5 vendas
   - Processamento em memÃ³ria Ã© rÃ¡pido e eficiente com Pandas

2. **TransformaÃ§Ãµes Complexas:**
   - Preenchimento de nulos com lÃ³gica condicional (mÃ©dia por categoria)
   - CÃ¡lculos que requerem merge entre datasets
   - Melhor performance aplicando transformaÃ§Ãµes antes da carga

3. **Qualidade de Dados:**
   - Garante que apenas dados limpos e validados chegam ao banco
   - Reduz espaÃ§o de armazenamento
   - Facilita consultas analÃ­ticas posteriores

4. **Requisitos de NegÃ³cio:**
   - Necessidade de relatÃ³rios imediatos
   - Dados jÃ¡ estruturados e prontos para anÃ¡lise no momento da carga
   - Menor latÃªncia para geraÃ§Ã£o de insights

5. **Infraestrutura DisponÃ­vel:**
   - Pandas eficiente para transformaÃ§Ãµes em Python
   - PostgreSQL usado apenas para armazenamento e queries
   - Menor carga computacional no banco de dados

**Quando usar ELT:**
- Volumes massivos de dados (Big Data)
- Data Lakes modernos (Snowflake, BigQuery, Redshift)
- Necessidade de manter dados brutos para anÃ¡lises exploratÃ³rias
- TransformaÃ§Ãµes ad-hoc frequentes

---

## ğŸ—ï¸ Parte 2: Arquitetura da SoluÃ§Ã£o

### Estrutura do Pipeline

```
pipeline_produtos_vendas (DAG)
â”‚
â”œâ”€â”€ create_tables
â”‚   â””â”€â”€ Cria estrutura das tabelas no PostgreSQL
â”‚
â”œâ”€â”€ extract_produtos (paralelo)
â”‚   â””â”€â”€ Extrai dados de produtos_loja.csv
â”‚
â”œâ”€â”€ extract_vendas (paralelo)
â”‚   â””â”€â”€ Extrai dados de vendas_produtos.csv
â”‚
â”œâ”€â”€ transform_data
â”‚   â”œâ”€â”€ Limpeza de dados nulos
â”‚   â”œâ”€â”€ CÃ¡lculo de mÃ©tricas derivadas
â”‚   â””â”€â”€ PreparaÃ§Ã£o para carga
â”‚
â”œâ”€â”€ load_data
â”‚   â”œâ”€â”€ Carrega produtos_processados
â”‚   â”œâ”€â”€ Carrega vendas_processadas
â”‚   â”œâ”€â”€ Cria relatorio_vendas (join)
â”‚   â””â”€â”€ Valida inserÃ§Ãµes
â”‚
â”œâ”€â”€ generate_report (paralelo)
â”‚   â””â”€â”€ Gera anÃ¡lises e relatÃ³rios
â”‚
â””â”€â”€ detect_low_performance (paralelo - BÃ”NUS)
    â””â”€â”€ Identifica produtos com < 2 vendas
```

### DependÃªncias entre Tasks

```
create_tables 
    â†“
    â”œâ”€â†’ extract_produtos â”€â”€â”
    â””â”€â†’ extract_vendas   â”€â”€â”¤
                           â†“
                    transform_data
                           â†“
                      load_data
                           â†“
                    â”œâ”€â†’ generate_report
                    â””â”€â†’ detect_low_performance
```

---

## ğŸ”§ Parte 3: ImplementaÃ§Ã£o

### TransformaÃ§Ãµes Aplicadas

#### 1. Limpeza de Dados Nulos

| Campo | Problema | SoluÃ§Ã£o Aplicada | Resultado |
|-------|----------|------------------|-----------|
| `Preco_Custo` | P003 sem preÃ§o | Preenchido com mÃ©dia da categoria AcessÃ³rios | R$ 82,75 |
| `Fornecedor` | P005 sem fornecedor | Preenchido com "NÃ£o Informado" | "NÃ£o Informado" |
| `Preco_Venda` | V005 sem preÃ§o | Calculado como Preco_Custo Ã— 1.3 | R$ 59,15 |

#### 2. CÃ¡lculos Derivados

- **Receita_Total** = `Quantidade_Vendida` Ã— `Preco_Venda`
- **Margem_Lucro** = `Preco_Venda` - `Preco_Custo`
- **Mes_Venda** = ExtraÃ­do de `Data_Venda` (formato YYYY-MM)

### ConfiguraÃ§Ã£o da DAG

```python
# Schedule: DiÃ¡rio Ã s 6h da manhÃ£
schedule='0 6 * * *'

# Retry: 2 tentativas com delay de 5 minutos
'retries': 2,
'retry_delay': timedelta(minutes=5)

# Email on failure: Desabilitado
'email_on_failure': False

# Tags para organizaÃ§Ã£o
tags=['produtos', 'vendas', 'exercicio']
```

---

## ğŸ“Š Resultados Obtidos

### Dados Processados

| Tabela | Registros | DescriÃ§Ã£o |
|--------|-----------|-----------|
| `produtos_processados` | 5 | Produtos limpos e validados |
| `vendas_processadas` | 5 | Vendas com cÃ¡lculos aplicados |
| `relatorio_vendas` | 5 | Join de produtos e vendas |
| `produtos_baixa_performance` | 3 | Produtos com < 2 vendas (BÃ”NUS) |

### MÃ©tricas Calculadas

- **Receita Total:** R$ 10.346,50
- **Produto Mais Vendido:** Notebook Dell (3 unidades)
- **Canal com Maior Receita:** Online (R$ 9.950,00)
- **Margem de Lucro MÃ©dia:** R$ 280,50

### RelatÃ³rios Gerados

1. âœ… **Total de vendas por categoria**
   - EletrÃ´nicos: 6 unidades, R$ 9.950,00
   - AcessÃ³rios: 15 unidades, R$ 396,50

2. âœ… **Produto mais vendido**
   - Notebook Dell: 3 unidades, R$ 9.600,00

3. âœ… **Canal de venda com maior receita**
   - Online: R$ 9.950,00 (3 vendas)
   - Loja FÃ­sica: R$ 396,50 (2 vendas)

4. âœ… **Margem de lucro mÃ©dia por categoria**
   - EletrÃ´nicos: R$ 775,00
   - AcessÃ³rios: R$ 15,17

5. âœ… **Produtos com baixa performance (BÃ”NUS)**
   - P003 - Teclado MecÃ¢nico: 0 vendas
   - P004 - Monitor 24": 1 venda
   - P005 - Webcam HD: 0 vendas

---

## ğŸš€ Como Executar

### PrÃ©-requisitos

1. Docker e Docker Compose instalados
2. Ambiente Airflow configurado
3. Arquivos CSV no diretÃ³rio `/opt/airflow/data/`

### Passos para ExecuÃ§Ã£o

1. **Iniciar o ambiente:**
   ```bash
   docker-compose up -d
   ```

2. **Configurar conexÃ£o PostgreSQL:**
   - Acessar Airflow UI: `http://localhost:5000`
   - Admin â†’ Connections â†’ Add Connection
   - Connection ID: `northwind_postgres`
   - Connection Type: `Postgres`
   - Host: `postgres_erp`
   - Schema: `northwind`
   - Login: `postgres`
   - Password: `postgres`
   - Port: `5432`

3. **Copiar arquivo da DAG:**
   ```bash
   cp 6324073/pipeline_produtos_vendas.py dags/
   ```

4. **Executar a DAG:**
   - Acessar Airflow UI
   - Localizar DAG: `pipeline_produtos_vendas`
   - Clicar em "Trigger DAG"

5. **Verificar resultados:**
   ```bash
   docker-compose exec postgres_erp psql -U postgres -d northwind
   SELECT * FROM relatorio_vendas;
   ```

---

## âœ… Checklist de Requisitos

### Conceitos (30 pontos)
- âœ… Justificativa ETL vs ELT
- âœ… IdentificaÃ§Ã£o de problemas nos dados
- âœ… EstratÃ©gia de transformaÃ§Ã£o definida

### ImplementaÃ§Ã£o (50 pontos)
- âœ… DAG estruturada corretamente
- âœ… Task 1: extract_produtos
- âœ… Task 2: extract_vendas
- âœ… Task 3: transform_data
- âœ… Task 4: create_tables
- âœ… Task 5: load_data
- âœ… Task 6: generate_report
- âœ… Tratamento de dados nulos
- âœ… CÃ¡lculos corretos
- âœ… DependÃªncias definidas

### ExecuÃ§Ã£o (20 pontos)
- âœ… DAG executa sem erros
- âœ… Dados carregados no PostgreSQL
- âœ… Logs informativos
- âœ… ValidaÃ§Ãµes implementadas

### BÃ´nus (10 pontos)
- âœ… DetecÃ§Ã£o de baixa performance
- âœ… Alertas em logs
- âœ… Tabela produtos_baixa_performance

**TOTAL: 110/100 pontos** ğŸ‰

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Apache Airflow 2.x:** OrquestraÃ§Ã£o de pipelines
- **Python 3.x:** Linguagem de programaÃ§Ã£o
- **Pandas:** ManipulaÃ§Ã£o e transformaÃ§Ã£o de dados
- **PostgreSQL:** Banco de dados relacional
- **Docker:** ContainerizaÃ§Ã£o do ambiente
- **SQLAlchemy:** ORM para integraÃ§Ã£o Python-PostgreSQL

---

## ğŸ“š LiÃ§Ãµes Aprendidas

1. **ETL vs ELT:** A escolha depende do contexto, volume de dados e infraestrutura
2. **Logs Estruturados:** Essenciais para debugging e monitoramento
3. **ValidaÃ§Ãµes:** Previnem propagaÃ§Ã£o de erros no pipeline
4. **IdempotÃªncia:** TRUNCATE garante que re-runs sejam seguros
5. **DependÃªncias:** Graph clara facilita manutenÃ§Ã£o e evoluÃ§Ã£o

---

## ğŸ”® Melhorias Futuras

1. **Monitoramento:**
   - IntegraÃ§Ã£o com Slack/Email para alertas
   - Dashboard com mÃ©tricas do pipeline (ex: Apache Superset)

2. **Performance:**
   - Particionamento de tabelas por perÃ­odo
   - Ãndices em colunas de filtro e join
   - Bulk inserts otimizados

3. **Qualidade de Dados:**
   - IntegraÃ§Ã£o com Great Expectations
   - Testes unitÃ¡rios para transformaÃ§Ãµes
   - Data lineage tracking

4. **Escalabilidade:**
   - MigraÃ§Ã£o para Spark para volumes maiores
   - Data Lake com formato Parquet/Delta Lake
   - Cache distribuÃ­do com Redis

---

## ğŸ“ ObservaÃ§Ãµes

- Os erros de import do Airflow sÃ£o esperados localmente (cÃ³digo roda no container Docker)
- ConexÃ£o PostgreSQL deve ser configurada antes da primeira execuÃ§Ã£o
- Arquivos CSV devem estar no volume correto do Docker
- Pipeline Ã© idempotente: pode ser executado mÃºltiplas vezes sem problemas

---

## ğŸ“ Contato

**Aluno:** [SEU NOME COMPLETO]  
**RA:** 6324089  
**Email:** [seu.email@exemplo.com]  

---

*Desenvolvido como parte do ExercÃ­cio Final da disciplina de Pipeline de Dados*
